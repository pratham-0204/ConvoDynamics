{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e07206-099e-4d31-852d-08ac35983b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   emotion                                             pixels     Usage\n",
      "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
      "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
      "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
      "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
      "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "df = pd.read_csv('fer2013.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "407f1983-7e35-4763-998d-2acb372028e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for index, row in df.iterrows():\n",
    "    k = row['pixels'].split(\" \")\n",
    "    if row['Usage'] == 'Training':\n",
    "        X_train.append(np.array(k))\n",
    "        y_train.append(row['emotion'])\n",
    "    elif row['Usage'] == 'PublicTest':\n",
    "        X_test.append(np.array(k))\n",
    "        y_test.append(row['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7b343a-6827-4e6a-921c-27ca5671cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train, dtype = 'uint8')\n",
    "y_train = np.array(y_train, dtype = 'uint8')\n",
    "X_test = np.array(X_test, dtype = 'uint8')\n",
    "y_test = np.array(y_test, dtype = 'uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e579f5-c00b-4f53-9922-50b697287107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "y_train= to_categorical(y_train, num_classes=7)\n",
    "y_test = to_categorical(y_test, num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "442b984b-af92-486c-81d6-be32477df820",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131fc19a-c92f-4eeb-8b05-c90f28829d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "datagen = ImageDataGenerator( \n",
    "    rescale=1./255,\n",
    "    rotation_range = 10,\n",
    "    horizontal_flip = True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode = 'nearest')\n",
    "testgen = ImageDataGenerator(rescale=1./255)\n",
    "datagen.fit(X_train)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "331630d8-8c84-43c6-855c-a59ffe824c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flow = datagen.flow(X_train, y_train, batch_size=batch_size) \n",
    "test_flow = testgen.flow(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d45325a0-0d17-4768-a9b2-f1976869f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import concatenate\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1, l2\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e58eda-8cd6-4f92-8a73-f44f17dc6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FER_Model(input_shape=(48,48,1)):\n",
    "    # first input model\n",
    "    visible = Input(shape=input_shape, name='input')\n",
    "    num_classes = 7\n",
    "    #the 1-st block\n",
    "    conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_1')(visible)\n",
    "    conv1_1 = BatchNormalization()(conv1_1)\n",
    "    conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_2')(conv1_1)\n",
    "    conv1_2 = BatchNormalization()(conv1_2)\n",
    "    pool1_1 = MaxPooling2D(pool_size=(2,2), name = 'pool1_1')(conv1_2)\n",
    "    drop1_1 = Dropout(0.3, name = 'drop1_1')(pool1_1)#the 2-nd block\n",
    "    conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_1')(drop1_1)\n",
    "    conv2_1 = BatchNormalization()(conv2_1)\n",
    "    conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_2')(conv2_1)\n",
    "    conv2_2 = BatchNormalization()(conv2_2)\n",
    "    conv2_3 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_3')(conv2_2)\n",
    "    conv2_2 = BatchNormalization()(conv2_3)\n",
    "    pool2_1 = MaxPooling2D(pool_size=(2,2), name = 'pool2_1')(conv2_3)\n",
    "    drop2_1 = Dropout(0.3, name = 'drop2_1')(pool2_1)#the 3-rd block\n",
    "    conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_1')(drop2_1)\n",
    "    conv3_1 = BatchNormalization()(conv3_1)\n",
    "    conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_2')(conv3_1)\n",
    "    conv3_2 = BatchNormalization()(conv3_2)\n",
    "    conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_3')(conv3_2)\n",
    "    conv3_3 = BatchNormalization()(conv3_3)\n",
    "    conv3_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_4')(conv3_3)\n",
    "    conv3_4 = BatchNormalization()(conv3_4)\n",
    "    pool3_1 = MaxPooling2D(pool_size=(2,2), name = 'pool3_1')(conv3_4)\n",
    "    drop3_1 = Dropout(0.3, name = 'drop3_1')(pool3_1)#the 4-th block\n",
    "    conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_1')(drop3_1)\n",
    "    conv4_1 = BatchNormalization()(conv4_1)\n",
    "    conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_2')(conv4_1)\n",
    "    conv4_2 = BatchNormalization()(conv4_2)\n",
    "    conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_3')(conv4_2)\n",
    "    conv4_3 = BatchNormalization()(conv4_3)\n",
    "    conv4_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_4')(conv4_3)\n",
    "    conv4_4 = BatchNormalization()(conv4_4)\n",
    "    pool4_1 = MaxPooling2D(pool_size=(2,2), name = 'pool4_1')(conv4_4)\n",
    "    drop4_1 = Dropout(0.3, name = 'drop4_1')(pool4_1)\n",
    "    \n",
    "    #the 5-th block\n",
    "    conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_1')(drop4_1)\n",
    "    conv5_1 = BatchNormalization()(conv5_1)\n",
    "    conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_2')(conv5_1)\n",
    "    conv5_2 = BatchNormalization()(conv5_2)\n",
    "    conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_3')(conv5_2)\n",
    "    conv5_3 = BatchNormalization()(conv5_3)\n",
    "    conv5_4 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_4')(conv5_3)\n",
    "    conv5_3 = BatchNormalization()(conv5_3)\n",
    "    pool5_1 = MaxPooling2D(pool_size=(2,2), name = 'pool5_1')(conv5_4)\n",
    "    drop5_1 = Dropout(0.3, name = 'drop5_1')(pool5_1)#Flatten and output\n",
    "    flatten = Flatten(name = 'flatten')(drop5_1)\n",
    "    ouput = Dense(num_classes, activation='softmax', name = 'output')(flatten)# create model \n",
    "    model = Model(inputs =visible, outputs = ouput)\n",
    "    # summary layers\n",
    "    print(model.summary())\n",
    "    ned\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77685c3e-0134-4568-a371-a855a92347ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 48, 48, 1)]       0         \n",
      "                                                                 \n",
      " conv1_1 (Conv2D)            (None, 48, 48, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization_85 (Ba  (None, 48, 48, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv1_2 (Conv2D)            (None, 48, 48, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_86 (Ba  (None, 48, 48, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " pool1_1 (MaxPooling2D)      (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " drop1_1 (Dropout)           (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " conv2_1 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_87 (Ba  (None, 24, 24, 128)       512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2_2 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_88 (Ba  (None, 24, 24, 128)       512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2_3 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " pool2_1 (MaxPooling2D)      (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " drop2_1 (Dropout)           (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv3_1 (Conv2D)            (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_90 (Ba  (None, 12, 12, 256)       1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv3_2 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_91 (Ba  (None, 12, 12, 256)       1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv3_3 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_92 (Ba  (None, 12, 12, 256)       1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv3_4 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_93 (Ba  (None, 12, 12, 256)       1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " pool3_1 (MaxPooling2D)      (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " drop3_1 (Dropout)           (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " conv4_1 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_94 (Ba  (None, 6, 6, 256)         1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv4_2 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_95 (Ba  (None, 6, 6, 256)         1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv4_3 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_96 (Ba  (None, 6, 6, 256)         1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv4_4 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_97 (Ba  (None, 6, 6, 256)         1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " pool4_1 (MaxPooling2D)      (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " drop4_1 (Dropout)           (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " conv5_1 (Conv2D)            (None, 3, 3, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_98 (Ba  (None, 3, 3, 512)         2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv5_2 (Conv2D)            (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_99 (Ba  (None, 3, 3, 512)         2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv5_3 (Conv2D)            (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_100 (B  (None, 3, 3, 512)         2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " conv5_4 (Conv2D)            (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " pool5_1 (MaxPooling2D)      (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " drop5_1 (Dropout)           (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 7)                 3591      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13111367 (50.02 MB)\n",
      "Trainable params: 13103431 (49.99 MB)\n",
      "Non-trainable params: 7936 (31.00 KB)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\legacy\\adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "model = FER_Model() \n",
    "opt = tf.keras.optimizers.legacy.Adam(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e31051e6-20e3-4927-a491-532710a8519f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prath\\AppData\\Local\\Temp\\ipykernel_1988\\2811330767.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_flow,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 310s 686ms/step - loss: 2.0448 - accuracy: 0.2103 - val_loss: 1.8958 - val_accuracy: 0.2494\n",
      "Epoch 2/100\n",
      "448/448 [==============================] - 314s 699ms/step - loss: 1.7971 - accuracy: 0.2476 - val_loss: 1.8052 - val_accuracy: 0.2552\n",
      "Epoch 3/100\n",
      "448/448 [==============================] - 322s 717ms/step - loss: 1.7647 - accuracy: 0.2674 - val_loss: 1.7410 - val_accuracy: 0.2906\n",
      "Epoch 4/100\n",
      "448/448 [==============================] - 326s 727ms/step - loss: 1.7264 - accuracy: 0.2987 - val_loss: 1.7063 - val_accuracy: 0.3243\n",
      "Epoch 5/100\n",
      "448/448 [==============================] - 303s 675ms/step - loss: 1.6702 - accuracy: 0.3349 - val_loss: 1.6108 - val_accuracy: 0.3867\n",
      "Epoch 6/100\n",
      "448/448 [==============================] - 305s 680ms/step - loss: 1.5913 - accuracy: 0.3771 - val_loss: 1.5519 - val_accuracy: 0.4149\n",
      "Epoch 7/100\n",
      "448/448 [==============================] - 298s 665ms/step - loss: 1.5252 - accuracy: 0.4078 - val_loss: 1.5154 - val_accuracy: 0.4567\n",
      "Epoch 8/100\n",
      "448/448 [==============================] - 293s 654ms/step - loss: 1.4491 - accuracy: 0.4382 - val_loss: 1.4170 - val_accuracy: 0.4840\n",
      "Epoch 9/100\n",
      "448/448 [==============================] - 296s 660ms/step - loss: 1.3950 - accuracy: 0.4666 - val_loss: 1.2951 - val_accuracy: 0.4946\n",
      "Epoch 10/100\n",
      "448/448 [==============================] - 296s 661ms/step - loss: 1.3355 - accuracy: 0.4911 - val_loss: 1.2419 - val_accuracy: 0.5210\n",
      "Epoch 11/100\n",
      "448/448 [==============================] - 296s 660ms/step - loss: 1.2879 - accuracy: 0.5107 - val_loss: 1.2497 - val_accuracy: 0.5169\n",
      "Epoch 12/100\n",
      "448/448 [==============================] - 298s 663ms/step - loss: 1.2447 - accuracy: 0.5266 - val_loss: 1.2673 - val_accuracy: 0.5157\n",
      "Epoch 13/100\n",
      "448/448 [==============================] - 299s 666ms/step - loss: 1.2195 - accuracy: 0.5383 - val_loss: 1.2705 - val_accuracy: 0.5180\n",
      "Epoch 14/100\n",
      "448/448 [==============================] - 309s 688ms/step - loss: 1.1806 - accuracy: 0.5523 - val_loss: 1.3580 - val_accuracy: 0.4726\n",
      "Epoch 15/100\n",
      "448/448 [==============================] - 313s 698ms/step - loss: 1.1548 - accuracy: 0.5646 - val_loss: 1.2338 - val_accuracy: 0.5294\n",
      "Epoch 16/100\n",
      "448/448 [==============================] - 311s 694ms/step - loss: 1.1342 - accuracy: 0.5718 - val_loss: 1.1243 - val_accuracy: 0.5662\n",
      "Epoch 17/100\n",
      "448/448 [==============================] - 312s 696ms/step - loss: 1.1033 - accuracy: 0.5811 - val_loss: 1.1290 - val_accuracy: 0.5812\n",
      "Epoch 18/100\n",
      "448/448 [==============================] - 312s 696ms/step - loss: 1.0854 - accuracy: 0.5894 - val_loss: 1.2575 - val_accuracy: 0.5302\n",
      "Epoch 19/100\n",
      "448/448 [==============================] - 313s 698ms/step - loss: 1.0634 - accuracy: 0.6005 - val_loss: 1.1392 - val_accuracy: 0.5676\n",
      "Epoch 20/100\n",
      "448/448 [==============================] - 312s 696ms/step - loss: 1.0501 - accuracy: 0.6035 - val_loss: 1.0621 - val_accuracy: 0.6099\n",
      "Epoch 21/100\n",
      "448/448 [==============================] - 313s 697ms/step - loss: 1.0241 - accuracy: 0.6137 - val_loss: 1.1226 - val_accuracy: 0.5851\n",
      "Epoch 22/100\n",
      "448/448 [==============================] - 307s 684ms/step - loss: 1.0115 - accuracy: 0.6196 - val_loss: 1.0409 - val_accuracy: 0.6250\n",
      "Epoch 23/100\n",
      "448/448 [==============================] - 302s 673ms/step - loss: 0.9959 - accuracy: 0.6279 - val_loss: 1.0369 - val_accuracy: 0.6160\n",
      "Epoch 24/100\n",
      "448/448 [==============================] - 312s 696ms/step - loss: 0.9787 - accuracy: 0.6318 - val_loss: 1.0108 - val_accuracy: 0.6317\n",
      "Epoch 25/100\n",
      "448/448 [==============================] - 306s 682ms/step - loss: 0.9677 - accuracy: 0.6384 - val_loss: 1.0707 - val_accuracy: 0.6088\n",
      "Epoch 26/100\n",
      "448/448 [==============================] - 303s 675ms/step - loss: 0.9472 - accuracy: 0.6420 - val_loss: 1.0635 - val_accuracy: 0.6030\n",
      "Epoch 27/100\n",
      "448/448 [==============================] - 303s 675ms/step - loss: 0.9358 - accuracy: 0.6526 - val_loss: 1.0039 - val_accuracy: 0.6369\n",
      "Epoch 28/100\n",
      "448/448 [==============================] - 301s 672ms/step - loss: 0.9204 - accuracy: 0.6545 - val_loss: 1.1035 - val_accuracy: 0.6024\n",
      "Epoch 29/100\n",
      "448/448 [==============================] - 301s 671ms/step - loss: 0.9107 - accuracy: 0.6578 - val_loss: 1.0094 - val_accuracy: 0.6261\n",
      "Epoch 30/100\n",
      "448/448 [==============================] - 304s 677ms/step - loss: 0.8943 - accuracy: 0.6688 - val_loss: 1.0665 - val_accuracy: 0.6133\n",
      "Epoch 31/100\n",
      "448/448 [==============================] - 302s 673ms/step - loss: 0.8862 - accuracy: 0.6668 - val_loss: 1.0335 - val_accuracy: 0.6250\n",
      "Epoch 32/100\n",
      "448/448 [==============================] - 300s 670ms/step - loss: 0.8690 - accuracy: 0.6748 - val_loss: 0.9780 - val_accuracy: 0.6367\n",
      "Epoch 33/100\n",
      "448/448 [==============================] - 302s 673ms/step - loss: 0.8630 - accuracy: 0.6775 - val_loss: 0.9747 - val_accuracy: 0.6389\n",
      "Epoch 34/100\n",
      "448/448 [==============================] - 300s 670ms/step - loss: 0.8486 - accuracy: 0.6829 - val_loss: 1.0252 - val_accuracy: 0.6244\n",
      "Epoch 35/100\n",
      "448/448 [==============================] - 315s 703ms/step - loss: 0.8346 - accuracy: 0.6874 - val_loss: 0.9573 - val_accuracy: 0.6553\n",
      "Epoch 36/100\n",
      "448/448 [==============================] - 425s 948ms/step - loss: 0.8284 - accuracy: 0.6909 - val_loss: 0.9910 - val_accuracy: 0.6456\n",
      "Epoch 37/100\n",
      "448/448 [==============================] - 387s 863ms/step - loss: 0.8120 - accuracy: 0.6948 - val_loss: 1.0226 - val_accuracy: 0.6392\n",
      "Epoch 38/100\n",
      "448/448 [==============================] - 297s 662ms/step - loss: 0.7997 - accuracy: 0.7005 - val_loss: 1.0095 - val_accuracy: 0.6428\n",
      "Epoch 39/100\n",
      "448/448 [==============================] - 300s 668ms/step - loss: 0.7901 - accuracy: 0.7073 - val_loss: 0.9876 - val_accuracy: 0.6425\n",
      "Epoch 40/100\n",
      "448/448 [==============================] - 301s 671ms/step - loss: 0.7801 - accuracy: 0.7100 - val_loss: 1.0276 - val_accuracy: 0.6434\n",
      "Epoch 41/100\n",
      "448/448 [==============================] - 300s 669ms/step - loss: 0.7623 - accuracy: 0.7145 - val_loss: 1.0191 - val_accuracy: 0.6403\n",
      "Epoch 42/100\n",
      "448/448 [==============================] - 304s 678ms/step - loss: 0.7559 - accuracy: 0.7183 - val_loss: 1.0138 - val_accuracy: 0.6475\n",
      "Epoch 43/100\n",
      "448/448 [==============================] - 305s 681ms/step - loss: 0.7431 - accuracy: 0.7225 - val_loss: 0.9879 - val_accuracy: 0.6500\n",
      "Epoch 44/100\n",
      "448/448 [==============================] - 304s 677ms/step - loss: 0.7240 - accuracy: 0.7309 - val_loss: 0.9778 - val_accuracy: 0.6565\n",
      "Epoch 45/100\n",
      "448/448 [==============================] - 304s 677ms/step - loss: 0.7232 - accuracy: 0.7295 - val_loss: 0.9892 - val_accuracy: 0.6584\n",
      "Epoch 46/100\n",
      "448/448 [==============================] - 360s 803ms/step - loss: 0.7077 - accuracy: 0.7365 - val_loss: 1.0057 - val_accuracy: 0.6573\n",
      "Epoch 47/100\n",
      "448/448 [==============================] - 430s 958ms/step - loss: 0.6982 - accuracy: 0.7396 - val_loss: 1.0099 - val_accuracy: 0.6559\n",
      "Epoch 48/100\n",
      "448/448 [==============================] - 431s 961ms/step - loss: 0.6906 - accuracy: 0.7421 - val_loss: 0.9807 - val_accuracy: 0.6709\n",
      "Epoch 49/100\n",
      "448/448 [==============================] - 430s 960ms/step - loss: 0.6813 - accuracy: 0.7467 - val_loss: 1.0038 - val_accuracy: 0.6648\n",
      "Epoch 50/100\n",
      "448/448 [==============================] - 398s 888ms/step - loss: 0.6690 - accuracy: 0.7501 - val_loss: 1.0475 - val_accuracy: 0.6520\n",
      "Epoch 51/100\n",
      "448/448 [==============================] - 305s 680ms/step - loss: 0.6565 - accuracy: 0.7542 - val_loss: 0.9730 - val_accuracy: 0.6645\n",
      "Epoch 52/100\n",
      "448/448 [==============================] - 306s 682ms/step - loss: 0.6484 - accuracy: 0.7609 - val_loss: 0.9972 - val_accuracy: 0.6676\n",
      "Epoch 53/100\n",
      "448/448 [==============================] - 302s 673ms/step - loss: 0.6323 - accuracy: 0.7646 - val_loss: 1.0369 - val_accuracy: 0.6656\n",
      "Epoch 54/100\n",
      "448/448 [==============================] - 375s 837ms/step - loss: 0.6270 - accuracy: 0.7678 - val_loss: 1.0429 - val_accuracy: 0.6637\n",
      "Epoch 55/100\n",
      "448/448 [==============================] - 431s 961ms/step - loss: 0.6172 - accuracy: 0.7743 - val_loss: 1.0148 - val_accuracy: 0.6707\n",
      "Epoch 56/100\n",
      "448/448 [==============================] - 429s 957ms/step - loss: 0.6119 - accuracy: 0.7733 - val_loss: 1.0738 - val_accuracy: 0.6595\n",
      "Epoch 57/100\n",
      "448/448 [==============================] - 430s 958ms/step - loss: 0.6030 - accuracy: 0.7753 - val_loss: 1.0311 - val_accuracy: 0.6737\n",
      "Epoch 58/100\n",
      "448/448 [==============================] - 430s 958ms/step - loss: 0.5897 - accuracy: 0.7784 - val_loss: 1.0927 - val_accuracy: 0.6595\n",
      "Epoch 59/100\n",
      "448/448 [==============================] - 316s 704ms/step - loss: 0.5787 - accuracy: 0.7849 - val_loss: 1.1270 - val_accuracy: 0.6584\n",
      "Epoch 60/100\n",
      "448/448 [==============================] - 310s 691ms/step - loss: 0.5673 - accuracy: 0.7878 - val_loss: 1.0981 - val_accuracy: 0.6643\n",
      "Epoch 61/100\n",
      "448/448 [==============================] - 309s 689ms/step - loss: 0.5605 - accuracy: 0.7906 - val_loss: 1.0840 - val_accuracy: 0.6673\n",
      "Epoch 62/100\n",
      "448/448 [==============================] - 304s 677ms/step - loss: 0.5540 - accuracy: 0.7933 - val_loss: 1.1031 - val_accuracy: 0.6606\n",
      "Epoch 63/100\n",
      "448/448 [==============================] - 305s 679ms/step - loss: 0.5517 - accuracy: 0.7972 - val_loss: 1.0506 - val_accuracy: 0.6656\n",
      "Epoch 64/100\n",
      "448/448 [==============================] - 398s 888ms/step - loss: 0.5362 - accuracy: 0.8006 - val_loss: 1.0518 - val_accuracy: 0.6662\n",
      "Epoch 65/100\n",
      "448/448 [==============================] - 430s 959ms/step - loss: 0.5325 - accuracy: 0.8020 - val_loss: 1.0801 - val_accuracy: 0.6771\n",
      "Epoch 66/100\n",
      "448/448 [==============================] - 430s 959ms/step - loss: 0.5182 - accuracy: 0.8073 - val_loss: 1.0856 - val_accuracy: 0.6790\n",
      "Epoch 67/100\n",
      "448/448 [==============================] - 432s 963ms/step - loss: 0.5122 - accuracy: 0.8108 - val_loss: 1.0595 - val_accuracy: 0.6743\n",
      "Epoch 68/100\n",
      "448/448 [==============================] - 322s 717ms/step - loss: 0.5027 - accuracy: 0.8141 - val_loss: 1.1274 - val_accuracy: 0.6670\n",
      "Epoch 69/100\n",
      "448/448 [==============================] - 308s 686ms/step - loss: 0.5018 - accuracy: 0.8124 - val_loss: 1.0743 - val_accuracy: 0.6723\n",
      "Epoch 70/100\n",
      "448/448 [==============================] - 305s 679ms/step - loss: 0.4857 - accuracy: 0.8201 - val_loss: 1.1463 - val_accuracy: 0.6701\n",
      "Epoch 71/100\n",
      "448/448 [==============================] - 304s 678ms/step - loss: 0.4824 - accuracy: 0.8210 - val_loss: 1.1511 - val_accuracy: 0.6746\n",
      "Epoch 72/100\n",
      "448/448 [==============================] - 304s 678ms/step - loss: 0.4729 - accuracy: 0.8262 - val_loss: 1.1707 - val_accuracy: 0.6612\n",
      "Epoch 73/100\n",
      "448/448 [==============================] - 380s 848ms/step - loss: 0.4672 - accuracy: 0.8277 - val_loss: 1.1425 - val_accuracy: 0.6645\n",
      "Epoch 74/100\n",
      "448/448 [==============================] - 429s 958ms/step - loss: 0.4614 - accuracy: 0.8298 - val_loss: 1.1326 - val_accuracy: 0.6662\n",
      "Epoch 75/100\n",
      "448/448 [==============================] - 431s 960ms/step - loss: 0.4499 - accuracy: 0.8318 - val_loss: 1.1725 - val_accuracy: 0.6807\n",
      "Epoch 76/100\n",
      "448/448 [==============================] - 432s 963ms/step - loss: 0.4422 - accuracy: 0.8350 - val_loss: 1.1862 - val_accuracy: 0.6785\n",
      "Epoch 77/100\n",
      "448/448 [==============================] - 430s 959ms/step - loss: 0.4359 - accuracy: 0.8383 - val_loss: 1.2037 - val_accuracy: 0.6698\n",
      "Epoch 78/100\n",
      "448/448 [==============================] - 431s 961ms/step - loss: 0.4285 - accuracy: 0.8426 - val_loss: 1.2050 - val_accuracy: 0.6726\n",
      "Epoch 79/100\n",
      "448/448 [==============================] - 432s 963ms/step - loss: 0.4239 - accuracy: 0.8432 - val_loss: 1.2353 - val_accuracy: 0.6651\n",
      "Epoch 80/100\n",
      "448/448 [==============================] - 431s 961ms/step - loss: 0.4161 - accuracy: 0.8458 - val_loss: 1.2428 - val_accuracy: 0.6651\n",
      "Epoch 81/100\n",
      "448/448 [==============================] - 431s 961ms/step - loss: 0.4120 - accuracy: 0.8457 - val_loss: 1.2355 - val_accuracy: 0.6651\n",
      "Epoch 82/100\n",
      "448/448 [==============================] - 339s 754ms/step - loss: 0.4053 - accuracy: 0.8496 - val_loss: 1.1737 - val_accuracy: 0.6746\n",
      "Epoch 83/100\n",
      "448/448 [==============================] - 2214s 5s/step - loss: 0.3955 - accuracy: 0.8532 - val_loss: 1.2142 - val_accuracy: 0.6721\n",
      "Epoch 84/100\n",
      "448/448 [==============================] - 295s 658ms/step - loss: 0.3884 - accuracy: 0.8549 - val_loss: 1.2564 - val_accuracy: 0.6687\n",
      "Epoch 85/100\n",
      "448/448 [==============================] - 294s 655ms/step - loss: 0.3839 - accuracy: 0.8572 - val_loss: 1.2252 - val_accuracy: 0.6782\n",
      "Epoch 86/100\n",
      "448/448 [==============================] - 296s 660ms/step - loss: 0.3850 - accuracy: 0.8592 - val_loss: 1.2860 - val_accuracy: 0.6743\n",
      "Epoch 87/100\n",
      "448/448 [==============================] - 298s 664ms/step - loss: 0.3763 - accuracy: 0.8634 - val_loss: 1.3361 - val_accuracy: 0.6659\n",
      "Epoch 88/100\n",
      "448/448 [==============================] - 297s 663ms/step - loss: 0.3732 - accuracy: 0.8596 - val_loss: 1.3241 - val_accuracy: 0.6659\n",
      "Epoch 89/100\n",
      "448/448 [==============================] - 305s 680ms/step - loss: 0.3632 - accuracy: 0.8649 - val_loss: 1.2945 - val_accuracy: 0.6640\n",
      "Epoch 90/100\n",
      "448/448 [==============================] - 308s 687ms/step - loss: 0.3557 - accuracy: 0.8684 - val_loss: 1.2827 - val_accuracy: 0.6701\n",
      "Epoch 91/100\n",
      "448/448 [==============================] - 306s 681ms/step - loss: 0.3499 - accuracy: 0.8713 - val_loss: 1.2925 - val_accuracy: 0.6712\n",
      "Epoch 92/100\n",
      "448/448 [==============================] - 302s 674ms/step - loss: 0.3481 - accuracy: 0.8718 - val_loss: 1.3211 - val_accuracy: 0.6701\n",
      "Epoch 93/100\n",
      "448/448 [==============================] - 302s 673ms/step - loss: 0.3414 - accuracy: 0.8705 - val_loss: 1.2707 - val_accuracy: 0.6732\n",
      "Epoch 94/100\n",
      "448/448 [==============================] - 302s 674ms/step - loss: 0.3394 - accuracy: 0.8727 - val_loss: 1.2983 - val_accuracy: 0.6698\n",
      "Epoch 95/100\n",
      "448/448 [==============================] - 303s 675ms/step - loss: 0.3266 - accuracy: 0.8771 - val_loss: 1.3590 - val_accuracy: 0.6737\n",
      "Epoch 96/100\n",
      "448/448 [==============================] - 303s 676ms/step - loss: 0.3252 - accuracy: 0.8788 - val_loss: 1.4182 - val_accuracy: 0.6615\n",
      "Epoch 97/100\n",
      "448/448 [==============================] - 303s 675ms/step - loss: 0.3207 - accuracy: 0.8818 - val_loss: 1.3915 - val_accuracy: 0.6695\n",
      "Epoch 98/100\n",
      "448/448 [==============================] - 302s 674ms/step - loss: 0.3174 - accuracy: 0.8839 - val_loss: 1.3422 - val_accuracy: 0.6662\n",
      "Epoch 99/100\n",
      "448/448 [==============================] - 304s 678ms/step - loss: 0.3173 - accuracy: 0.8837 - val_loss: 1.3744 - val_accuracy: 0.6729\n",
      "Epoch 100/100\n",
      "448/448 [==============================] - 302s 674ms/step - loss: 0.3072 - accuracy: 0.8854 - val_loss: 1.3919 - val_accuracy: 0.6734\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100  \n",
    "history = model.fit_generator(train_flow, \n",
    "                    steps_per_epoch=len(X_train) / batch_size, \n",
    "                    epochs=num_epochs,  \n",
    "                    verbose=1,  \n",
    "                    validation_data=test_flow,\n",
    "                    validation_steps=len(X_test) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba872100-312a-4057-a557-4f3f731eb88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
